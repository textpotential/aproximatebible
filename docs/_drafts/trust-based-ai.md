Why bother with trust? Is it even a force worth considering or is it simply a story we tell ourselves and each other? With Catherine Malabou, we agree that the emergence of AI in all areas of life and society demands a reinvention of trust (Shred translating Malabou, *Morphing Intelligence*, 163). Rather than build an approach to AI on a set of principles decided upon by a select few, trust suggests a network of relationships that are constantly negotiated. 

As a theological school, we have spent the past 125 years investing in the pursuit of human flourishing and building communities of trust. This history positions us well to participate in the emerging collective intelligence (Malabou) that can facilitate trust-based practices in our AI ecosystem. 

Integrate.ai has done some excellent work with their agile ethics framework to introduce trust-based practices into each phase of the machine learning life cycle

What do we mean by trust

*Transparent* - Are your models explainable and your data/development practices communicated to all stakeholders?
*Responsible* - Does your use and development of AI work to increase social good? Are you aware of the impact, both actual and possible, of your development on different sectors of society?
*User-centered* - Do you involve the needs and input of users in all appropriate stages of your AI workflows? Is there a feedback mechanism for users to shape the design and implementation of AI?
*Sustainable* - Are your AI practices agile enough to respond to new technologies, new social demands, and new legislation (GDPR)?
*Tangible* - Are your practices ...

With whom does trust matter? 

Customers
Employees/Partners
Investors


## why trust?

> It seems to me that today artificial intelligence represents the fourth narcissistic blow to humanity....First Copernicus, followed by Darwin, then psychoanalysis, and now the fourth blow: the capturing of intelligence by its own simulation, exceeding and transcending it. To recover from this kind of a blow requires first that it be accepted rather than denied. This is not to resign oneself to it but rather to reinvent trust. 
	Shred translating Malabou (*Morphing Intelligence*, 163)

### trust is the bottom line

The need for trust is not new in business ventures. Yet, with the rise of AI, the changes in our communication practices, and the proliferation of data and information, the practices supporting trust in core constituencies is changing. As we build and implement AI systems, we need a framework that will cultivate greater customer, employee, and investor trust, thus providing more sustainable revenues.

### trust goes beyond ethics

Much of the conversation around AI and society today focuses in on ethics (some examples). No doubt, ethics are important and we need people thinking about the principles that guide our use and development of AI. Yet, the practices developed around ethics and AI too easily get reduced to issues of compliance and litigation (as we learned from Mira Lane at Microsoft, in the majority of large tech companies, the ethics team is located in the legal department or proxied to an ethics board). A trust based framework for AI demands that we take a whole lifecycle approach to develop dispositions and practices toward trust at every stage of the AI development process, from problem definition to deployment and maintenance (reference integrate.ai's work here). This embedded trust approach involves the whole community in the task of developing an ecosystem of trust that will more proactively and systemically  shape the development of AI systems that will cultivate customer and investor trust.

For these reasons, we propose our TRUST or T2RUS framework for AI development.

## Transparent

* do all parties know how/when their data is being used?
* is the presence/activity of an AI system indicated to user or hidden?
* datasheets - are your data collection and preparation practices for each dataset made available?
* is your model interrogatable? move from explainability/intepretability debate to transparency through translation. i.e. can you translate the process by which your system made a decision? 
* are you clear about your bias management procedures? 
* are your assumptions articulated at each stage of the process? (e.g. irrelevant features in the dataset, proxies used)


## Responsibile

* toward social good
* building a future based on our values not simply on our past (data) - SELECTIVE BIAS!!
* do you have a process for anticipating unintended consequences?
* how do you handle when your model/system does harm? 
* is there any governance needed for your system?
* is privacy protected at all stages of the life-cycle

## User-centered

* learn from UX movement here, but go beyond persona work (UX) to stakeholder work (Mira)
* are users involved in design in impactful ways? 
* what are the user feedback loops?
* can users decide how and when their data is used?
* can users request their data extracted from model? 

## Sustainable

* are we fostering sustainable relationships between humans and machines? (e.g. is our development dependent on human dominion or how do we learn from machines?)
* environmental impact, energy, etc.
* agile enough to respond to new legislation or social demands(GDPR)
* agile enough to adapt to new technologies (e.g. PyTorch or TPUs)
* do we have the needed workforce to support your development? If not, how are you tooling?
* do you have a plan to address model drift (maintenance plan)


## Together

* Human/Human and Human/Machine
* Trust is fundamentally relational, it is an ongoing negotiation among participants. 
* Collaborative from the ground up - Human/Human 
* Demands that we consider machines as partners
* Cross-functional teams are essential to addressing the vast complex nature of issues and possibilities that may arise in AI development
* Diverse and inclusive teams will produce better AI systems